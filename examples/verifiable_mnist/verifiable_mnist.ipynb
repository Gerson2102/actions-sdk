{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build a Verifiable Neural Network with Giza Actions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Giza Actions provides developers with the tools to easily create and expand Verifiable Machine Learning solutions, transforming their Python scripts and ML models into robust, repeatable workflows. Models developed using the Action SDK possess a verifiable property, enabling you to encapsulate your model within a Zero-Knowledge cryptographic layer, thereby ensuring the integrity of the inference.\n",
    "\n",
    "In this tutorial, we will explore the process of building your first Neural Network using MNIST dataset, [Pytorch](https://pytorch.org/), and Giza Action SDK and demonstrating its verifiability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What is MNIST dataset?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The MNIST dataset is an extensive collection of handwritten digits, very popular in the field of image processing. Often, it's used as a reference point for machine learning algorithms. This dataset conveniently comes already partitioned into training and testing sets, a feature we'll delve into later in this tutorial.\n",
    "\n",
    "The MNIST database comprises a collection of 70,000 images of handwritten digits, ranging from 0 to 9. Each image measures 28 x 28 pixels. For the purpose of this tutorial, we will resize image to 14 x 14 pixels.\n",
    "\n",
    "![MNIST Dataset illustration](./imgs/mnist_dataset_illustration.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Login to Giza and create a Workspace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://api-dev.gizatech.xyz\n"
     ]
    }
   ],
   "source": [
    "# TODO: remove this cell when it deploys on main branch\n",
    "\n",
    "import os\n",
    "import uuid\n",
    "\n",
    "os.environ['GIZA_API_HOST'] = 'https://api-dev.gizatech.xyz'\n",
    "print(os.environ['GIZA_API_HOST'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we begin, it's important to note that since we will be using Giza tools, you need to log in to the Giza platform. To do this, we recommend following these steps to install the Giza CLI, create a user, and generate API keys:\n",
    "\n",
    "```bash\n",
    "$ pipx install giza-cli\n",
    "$ giza users create\n",
    "$ giza users login\n",
    "$ giza users create-api-key\n",
    "```\n",
    "\n",
    "For more detailed information about `login`, please refer to the [Giza-CLI documentation](https://cli.gizatech.xyz/welcome/readme)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you haven't already created a workspace associated with your user, you'll need to do so. Workspaces in our platform are a crucial component designed to enhance user interaction with Giza Actions. These workspaces provide a user-friendly interface (UI) for managing and tracking runs, tasks, and metadata associated with action executions.\n",
    "\n",
    "Please note that the workspace creation process can take up to 10 minutes as isolated resources are set up for each respective workspace. You can create a workspace using the following command:\n",
    "\n",
    "```bash\n",
    "giza workspaces create\n",
    "```\n",
    "\n",
    "If you've previously created a workspace with your account, you can retrieve your workspace URL as follows:\n",
    "\n",
    "```bash\n",
    "giza workspaces get\n",
    "```\n",
    "\n",
    "For more detailed information about `workspaces`, please refer to the [Workspace section of Giza-CLI documentation](https://cli.gizatech.xyz/resources/workspaces)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define your model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To begin, we must define the architecture of our model. In this tutorial, we will construct a basic feedforward neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import numpy as np\n",
    "import logging\n",
    "from scipy.ndimage import zoom\n",
    "from giza_actions.action import action, Action\n",
    "from giza_actions.task import task\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 196  # 14x14\n",
    "hidden_size = 10 \n",
    "num_classes = 10\n",
    "num_epochs = 10\n",
    "batch_size = 256\n",
    "learning_rate = 0.001\n",
    "\n",
    "class NeuralNet(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_classes):\n",
    "        super(NeuralNet, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.l1 = nn.Linear(input_size, hidden_size) \n",
    "        self.relu = nn.ReLU()\n",
    "        self.l2 = nn.Linear(hidden_size, num_classes) \n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.l1(x)\n",
    "        out = self.relu(out)\n",
    "        out = self.l2(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare your Tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the Giza Action SDK, a Task serves as a functional representation of a specific work segment within a Giza Actions workflow. Tasks offer a means to encapsulate portions of your workflow logic in traceable and reusable units across actions. Essentially, a Giza Actions task can accomplish nearly anything a typical Python function can. The unique aspect of tasks is their ability to access information about upstream dependencies and the status of these dependencies before starting. This capability enables tasks, for instance, to wait for the completion of another task before starting. Moreover, tasks benefit from Giza Actions' automated logging, which captures comprehensive details of each task run, including its duration, tags, and final state.\n",
    "\n",
    "This is how we will encapsulate the methods we build using the `@task` decorator."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare datasets and create loaders\n",
    "We need to download datasets and create loarders for both training and testing purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resize_images(images):\n",
    "    return np.array([zoom(image[0], (0.5, 0.5)) for image in images])\n",
    "\n",
    "@task(name=f'Prepare Datasets {uuid.uuid4()}')\n",
    "def prepare_datasets():\n",
    "    print(\"Prepare dataset...\")\n",
    "    train_dataset = torchvision.datasets.MNIST(root='./data', train=True, download=True)\n",
    "    test_dataset = torchvision.datasets.MNIST(root='./data', train=False)\n",
    "\n",
    "    x_train = resize_images(train_dataset)\n",
    "    x_test = resize_images(test_dataset)\n",
    "\n",
    "    x_train = torch.tensor(x_train.reshape(-1, 14*14).astype('float32') / 255)\n",
    "    y_train = torch.tensor([label for _, label in train_dataset], dtype=torch.long)\n",
    "\n",
    "    x_test = torch.tensor(x_test.reshape(-1, 14*14).astype('float32') / 255)\n",
    "    y_test = torch.tensor([label for _, label in test_dataset], dtype=torch.long)\n",
    "\n",
    "    print(\"✅ Datasets prepared successfully\")\n",
    "\n",
    "    return x_train, y_train, x_test, y_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "@task(name=f'Create Loaders {uuid.uuid4()}')\n",
    "def create_data_loaders(x_train, y_train, x_test, y_test):\n",
    "    print(\"Create loaders...\")\n",
    "\n",
    "    train_loader = DataLoader(TensorDataset(x_train, y_train), batch_size=batch_size, shuffle=True)\n",
    "    test_loader = DataLoader(TensorDataset(x_test, y_test), batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    print(\"✅ Loaders created!\")\n",
    "\n",
    "    return train_loader, test_loader\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the model\n",
    "\n",
    "We need to define our training method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "@task(name=f'Train model {uuid.uuid4()}')\n",
    "def train_model(train_loader):\n",
    "    print(\"Train model...\")\n",
    "\n",
    "    model = NeuralNet(input_size, hidden_size, num_classes).to(device)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        for i, (images, labels) in enumerate(train_loader):\n",
    "            images = images.to(device).reshape(-1, 14*14)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            if (i + 1) % 100 == 0:\n",
    "                print(f'Epoch [{epoch + 1}/{num_epochs}], Step [{i + 1}/{len(train_loader)}], Loss: {loss.item():.4f}')\n",
    "\n",
    "    print(\"✅ Model trained successfully\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test model\n",
    "\n",
    "We need to define our testing method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "@task(name=f'Test model {uuid.uuid4()}')\n",
    "def test_model(model, test_loader):\n",
    "    print(\"Test model...\")\n",
    "    with torch.no_grad():\n",
    "        n_correct = 0\n",
    "        n_samples = 0\n",
    "        for images, labels in test_loader:\n",
    "            images = images.to(device).reshape(-1, 14*14)\n",
    "            labels = labels.to(device)\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            n_samples += labels.size(0)\n",
    "            n_correct += (predicted == labels).sum().item()\n",
    "\n",
    "        acc = 100.0 * n_correct / n_samples\n",
    "        print(f'Accuracy of the network on the 10000 test images: {acc} %')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Action the tasks\n",
    "\n",
    "Now that we've prepared the taks, we need to execute them. This will be done using the `action` decorator."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Think of actions as unique types of functions. They are capable of receiving inputs, executing tasks, and producing outputs. Remarkably, transforming any standard function into a Giza Actions action is as simple as appending the `@action` decorator. This transformation alters the function's characteristics, bestowing several advantages:\n",
    "\n",
    "- Each time this function is used, its activity is monitored, with every state change communicated to the API for efficient tracking of the action's execution. \n",
    "  \n",
    "- Input parameters undergo automatic type verification and adaptation to ensure they match the required types. \n",
    "  \n",
    "- In cases of failure, the system is equipped to retry. Time constraints can be imposed to avert excessively prolonged workflows.\n",
    "  \n",
    "- Actions leverage inherent logging capabilities, which record essential details of each action run, including its duration and conclusive state.\n",
    "\n",
    "For more detailed information about `actions`, please refer to the [Actions section](https://actions.gizatech.xyz/welcome/giza-actions-sdk) of Action SDK documentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prepare dataset...\n",
      "✅ Datasets prepared successfully\n",
      "Create loaders...\n",
      "✅ Loaders created!\n",
      "Train model...\n",
      "Epoch [1/10], Step [100/235], Loss: 1.8091\n",
      "Epoch [1/10], Step [200/235], Loss: 1.1378\n",
      "Epoch [2/10], Step [100/235], Loss: 0.7580\n",
      "Epoch [2/10], Step [200/235], Loss: 0.5322\n",
      "Epoch [3/10], Step [100/235], Loss: 0.5158\n",
      "Epoch [3/10], Step [200/235], Loss: 0.5252\n",
      "Epoch [4/10], Step [100/235], Loss: 0.3461\n",
      "Epoch [4/10], Step [200/235], Loss: 0.3616\n",
      "Epoch [5/10], Step [100/235], Loss: 0.3054\n",
      "Epoch [5/10], Step [200/235], Loss: 0.3283\n",
      "Epoch [6/10], Step [100/235], Loss: 0.3933\n",
      "Epoch [6/10], Step [200/235], Loss: 0.3382\n",
      "Epoch [7/10], Step [100/235], Loss: 0.4457\n",
      "Epoch [7/10], Step [200/235], Loss: 0.2492\n",
      "Epoch [8/10], Step [100/235], Loss: 0.3833\n",
      "Epoch [8/10], Step [200/235], Loss: 0.2938\n",
      "Epoch [9/10], Step [100/235], Loss: 0.4024\n",
      "Epoch [9/10], Step [200/235], Loss: 0.2159\n",
      "Epoch [10/10], Step [100/235], Loss: 0.4806\n",
      "Epoch [10/10], Step [200/235], Loss: 0.2324\n",
      "✅ Model trained successfully\n",
      "Test model...\n",
      "Accuracy of the network on the 10000 test images: 91.84 %\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Completed(message=None, type=COMPLETED, result=UnpersistedResult(type='unpersisted', artifact_type='result', artifact_description='Unpersisted result of type `tuple`')),\n",
       " Completed(message=None, type=COMPLETED, result=UnpersistedResult(type='unpersisted', artifact_type='result', artifact_description='Unpersisted result of type `tuple`')),\n",
       " Completed(message=None, type=COMPLETED, result=UnpersistedResult(type='unpersisted', artifact_type='result', artifact_description='Unpersisted result of type `NeuralNet`')),\n",
       " Completed(message=None, type=COMPLETED, result=UnpersistedResult(type='unpersisted', artifact_type='result', artifact_description='Unpersisted result of type `NoneType`'))]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "@action(name=f'Execution {uuid.uuid4()}', )\n",
    "def execution():\n",
    "    x_train, y_train, x_test, y_test = prepare_datasets()\n",
    "    train_loader, test_loader = create_data_loaders(\n",
    "        x_train, y_train, x_test, y_test)\n",
    "    model = train_model(train_loader)\n",
    "    test_model(model, test_loader)\n",
    "\n",
    "\n",
    "execution()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When you return to your workspace and refresh it, you will observe your action currently in the execution process.\n",
    "\n",
    "To view all the actions you have run, navigate to the Action Runs section within your workspace. Each action run represents a single occurrence of executing the action.\n",
    "\n",
    "![Action runs](./imgs/action_runs.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Furthermore, by clicking on a particular action, you can closely monitor its progress. You'll have the ability to view logs, individual tasks, and associated metadata.\n",
    "\n",
    "![Action](./imgs/action.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deploy your action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Giza Action enables you to deploy workflows, turning them from manual activation into API-managed entities that can be triggered remotely. Once a workflow is deployed, it creates a deployment within your Giza Workspace, which remains active, awaiting action runs associated with that deployment. When a run is detected, it is asynchronously executed in a local subprocess.\n",
    "\n",
    "All your deployments can be found in the 'Deployments' section of your Workspace.\n",
    "\n",
    "![Deployment](./imgs/deployments.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/s3/6c0gmns50x36dt6vvfhv6jhc0000gn/T/ipykernel_16225/3902433836.py:12: RuntimeWarning: coroutine 'Action.serve' was never awaited\n",
      "  action_deploy.serve(name=\"pytorch-mnist-deployment\")\n",
      "RuntimeWarning: Enable tracemalloc to get the object allocation traceback\n"
     ]
    }
   ],
   "source": [
    "@action()\n",
    "def execution():\n",
    "    x_train, y_train, x_test, y_test = prepare_datasets()\n",
    "    train_loader, test_loader = create_data_loaders(\n",
    "        x_train, y_train, x_test, y_test)\n",
    "    model = train_model(train_loader)\n",
    "    test_model(model, test_loader)\n",
    "\n",
    "# This can only be executed in a Python script, not in a notebook\n",
    "if __name__ == '__main__':\n",
    "    action_deploy = Action(entrypoint=execution, name=\"pytorch-mnist-action\")\n",
    "    action_deploy.serve(name=\"pytorch-mnist-deployment\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run and Prove!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Up to this point, we have primarily focused on training and testing our model using PyTorch while monitoring its execution through the Giza platform. However, if you are here, it's likely because you want to harness the capabilities of ZKML (Zero-Knowledge Machine Learning) and have the ability to demonstrate the integrity of your model's inferences.\n",
    "\n",
    "In this section, we will delve into what it means to prove the integrity of inferences, what setup is required to make this possible, and how to verify the proof."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ZKML leverages validity proofs like SNARKs and STARKs, which enables the verification of the correctness of computational processes. By deploying such proof systems in machine learning applications, we gain the ability to validate the inference of ML models or to confirm that a specific input produced a certain output with a given model.\n",
    "\n",
    "To generate ZK proofs for your model inferences, you must first convert your model into ZK circuits. This conversion process involves leveraging programming languages that specialize in building ZK circuits, such as [Cairo-lang](https://www.cairo-lang.org/). Subsequently, using the Giza-CLI, you can transpile your model from ONNX to Cairo. This process will be covered in the upcoming sections.\n",
    "\n",
    "It's worth mentioning that at present, Orion and Action-SDK exclusively supports Cairo as a ZK backend. However, we are actively working on expanding support for other ZK backends (e.g; EZKL, Noir ...).\n",
    "\n",
    "![Giza Stack](./imgs/giza_stack.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert to ONNX\n",
    "\n",
    "Before invoking the Giza transpiler to convert your model into Cairo, you must first ensure that your model is converted to ONNX. We will explore this process in the following section.\n",
    "\n",
    "ONNX, short for Open Neural Network Exchange, is an open format for representing and exchanging machine learning models between different frameworks and libraries. It serves as an intermediary format that allows you to move models seamlessly between various platforms and tools, facilitating interoperability and flexibility in the machine learning ecosystem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/raphaeldoukhan/Library/Caches/pypoetry/virtualenvs/giza-actions-mYf3m_Lk-py3.11/lib/python3.11/site-packages/prefect/flows.py:338: UserWarning: A flow named 'Action: Convert To ONNX' and defined at '/Users/raphaeldoukhan/Desktop/Orion-Giza/Tools/actions-sdk/giza_actions/action.py:12' conflicts with another flow. Consider specifying a unique `name` parameter in the flow definition:\n",
      "\n",
      " `@flow(name='my_unique_name', ...)`\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prepare dataset...\n",
      "✅ Datasets prepared successfully\n",
      "Create loaders...\n",
      "✅ Loaders created!\n",
      "Train model...\n",
      "Epoch [1/10], Step [100/235], Loss: 1.7943\n",
      "Epoch [1/10], Step [200/235], Loss: 1.1904\n",
      "Epoch [2/10], Step [100/235], Loss: 0.7732\n",
      "Epoch [2/10], Step [200/235], Loss: 0.6201\n",
      "Epoch [3/10], Step [100/235], Loss: 0.4574\n",
      "Epoch [3/10], Step [200/235], Loss: 0.3970\n",
      "Epoch [4/10], Step [100/235], Loss: 0.3728\n",
      "Epoch [4/10], Step [200/235], Loss: 0.3808\n",
      "Epoch [5/10], Step [100/235], Loss: 0.4304\n",
      "Epoch [5/10], Step [200/235], Loss: 0.4226\n",
      "Epoch [6/10], Step [100/235], Loss: 0.2514\n",
      "Epoch [6/10], Step [200/235], Loss: 0.3149\n",
      "Epoch [7/10], Step [100/235], Loss: 0.2774\n",
      "Epoch [7/10], Step [200/235], Loss: 0.3136\n",
      "Epoch [8/10], Step [100/235], Loss: 0.3052\n",
      "Epoch [8/10], Step [200/235], Loss: 0.2545\n",
      "Epoch [9/10], Step [100/235], Loss: 0.2400\n",
      "Epoch [9/10], Step [200/235], Loss: 0.2815\n",
      "Epoch [10/10], Step [100/235], Loss: 0.2724\n",
      "Epoch [10/10], Step [200/235], Loss: 0.2534\n",
      "✅ Model trained successfully\n",
      "Test model...\n",
      "Accuracy of the network on the 10000 test images: 91.49 %\n",
      "Model has been converted to ONNX and saved as mnist_model.onnx\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Completed(message=None, type=COMPLETED, result=UnpersistedResult(type='unpersisted', artifact_type='result', artifact_description='Unpersisted result of type `tuple`')),\n",
       " Completed(message=None, type=COMPLETED, result=UnpersistedResult(type='unpersisted', artifact_type='result', artifact_description='Unpersisted result of type `tuple`')),\n",
       " Completed(message=None, type=COMPLETED, result=UnpersistedResult(type='unpersisted', artifact_type='result', artifact_description='Unpersisted result of type `NeuralNet`')),\n",
       " Completed(message=None, type=COMPLETED, result=UnpersistedResult(type='unpersisted', artifact_type='result', artifact_description='Unpersisted result of type `NoneType`')),\n",
       " Completed(message=None, type=COMPLETED, result=UnpersistedResult(type='unpersisted', artifact_type='result', artifact_description='Unpersisted result of type `NoneType`'))]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.onnx\n",
    "\n",
    "@task(name=f'Convert To ONNX {uuid.uuid4()}')\n",
    "def convert_to_onnx(model, onnx_file_path):\n",
    "    dummy_input = torch.randn(1, input_size).to(device)\n",
    "    torch.onnx.export(model, dummy_input, onnx_file_path,\n",
    "                      export_params=True, opset_version=10, do_constant_folding=True)\n",
    "\n",
    "    print(f\"Model has been converted to ONNX and saved as {onnx_file_path}\")\n",
    "\n",
    "\n",
    "@action(name=\"Action: Convert To ONNX\", )\n",
    "def execution():\n",
    "    x_train, y_train, x_test, y_test = prepare_datasets()\n",
    "    train_loader, test_loader = create_data_loaders(\n",
    "        x_train, y_train, x_test, y_test)\n",
    "    model = train_model(train_loader)\n",
    "    test_model(model, test_loader)\n",
    "\n",
    "    # Convert to ONNX\n",
    "    onnx_file_path = \"mnist_model.onnx\"\n",
    "    convert_to_onnx(model, onnx_file_path)\n",
    "\n",
    "\n",
    "execution()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transpile your model to Cairo and deploy on Giza"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we converted to ONNX, use the Giza-CLI to transpile your model to Orion Cairo code. \n",
    "\n",
    "```bash\n",
    "giza transpile mnist_model.onnx --output-path verifiable_mnist\n",
    "```\n",
    "\n",
    "After your model has been transpiled, it's ready for deployment on the Giza platform. Our platform's deployment process establishes services capable of accepting prediction requests through a specific endpoint. These services leverage Cairo under the hood to ensure provable inferences.\n",
    "\n",
    "To create a new service, users can employ the `deploy` command. The following command facilitates the deployment of a machine learning service ready to accept predictions at the `/cairo_run` endpoint.\n",
    "\n",
    "```bash\n",
    "giza deployments deploy --model-id 1 --version-id 1 inference.sierra\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to Obtain the Sierra File?\n",
    "\n",
    "To deploy a Cairo model, you will need to provide its Sierra file. Sierra is an intermediate representation between high level Cairo and compilation targets, such as CASM. To obtain this file, follow these steps:\n",
    "\n",
    "1. Navigate to your transpiled Cairo model directory:\n",
    "    ```bash\n",
    "    $ cd verifiable_mnist/inference\n",
    "    ```\n",
    "\n",
    "2. Compile your model using Scarb (make sure to have [Scarb](https://docs.swmansion.com/scarb/download.html) installed):\n",
    "\n",
    "    ```bash\n",
    "    $ scarb build\n",
    "    ```\n",
    "\n",
    "3. The Sierra file can be found in the generated directory at `target/dev/inference.sierra`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please keep in mind that if your model contains ONNX operators not currently supported by the Transpiler, the transpilation process will fail. We are actively working towards achieving 100% ONNX compatibility in both Orion and the Transpiler."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run your program\n",
    "\n",
    "Now that your Cairo model is deployed on the Giza platform, you have the capability to execute it. If you initiate a prediction using Giza Action without the `verifiable` mode, it runs the onnx version of the model.\n",
    "\n",
    "\n",
    "When you initiate a prediction using Giza Action in `verifiable` mode, it executes the Sierra program using CairoVM, generating trace and memory files for the proving. It also returns the output value and initiates a proving job to generate a Stark proof of the inference.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's make a prediction with the ONNX model (`veriable=False`) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  8.584354  -13.583519   -2.6759582   1.494859  -19.907509    5.371609\n",
      "   -9.260434   -4.6332397  -3.057909   -8.678334 ]]\n",
      "Predicted Digit: 0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from giza_actions.model import GizaModel\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "@task(name=f'Preprocess Image {uuid.uuid4()}')\n",
    "def preprocess_image(image_path):\n",
    "    from PIL import Image\n",
    "    import numpy as np\n",
    "\n",
    "    # Load image, convert to grayscale, resize and normalize\n",
    "    image = Image.open(image_path).convert('L')\n",
    "    # Resize to match the input size of the model\n",
    "    image = image.resize((14, 14))\n",
    "    image = np.array(image).astype('float32') / 255\n",
    "    image = image.reshape(1, 196)  # Reshape to (1, 196) for model input\n",
    "    return image\n",
    "\n",
    "\n",
    "@task(name=f'Prediction with ONNX {uuid.uuid4()}')\n",
    "def prediction(image):\n",
    "    model = GizaModel(model_path=\"./mnist_model.onnx\")\n",
    "\n",
    "    result = model.predict(\n",
    "        input_feed={\"onnx::Gemm_0\": image}, verifiable=False\n",
    "    )\n",
    "\n",
    "    # Convert result to a PyTorch tensor\n",
    "    result_tensor = torch.tensor(result)\n",
    "    # Apply softmax to convert to probabilities\n",
    "    probabilities = F.softmax(result_tensor, dim=1)\n",
    "    # Use argmax to get the predicted class\n",
    "    predicted_class = torch.argmax(probabilities, dim=1)\n",
    "\n",
    "    return predicted_class.item()\n",
    "\n",
    "\n",
    "@action(name=f'Execution: Prediction with ONNX {uuid.uuid4()}', )\n",
    "def execution():\n",
    "    image = preprocess_image(\"./zero.jpg\")\n",
    "    predicted_digit = prediction(image)\n",
    "    print(f\"Predicted Digit: {predicted_digit}\")\n",
    "\n",
    "    return predicted_digit\n",
    "\n",
    "\n",
    "execution()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's make a prediction with the Cairo model (`veriable=True`).\n",
    "\n",
    "DISCLAIMER: This section is still a work in progress and may not function correctly with certain Cairo programs. We are diligently working to resolve any issues and ensure it operates smoothly as soon as possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Starting deserialization process...\n",
      "✅ Deserialization completed! 🎉\n",
      "Result:  [ 1.02122345e+01 -1.45135193e+01 -1.16250610e+00  1.70765686e+00\n",
      " -1.23049927e+01  5.87187195e+00 -9.11688232e+00 -3.19342041e+00\n",
      " -1.10015869e-02 -7.16711426e+00]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 1.02122345e+01, -1.45135193e+01, -1.16250610e+00,  1.70765686e+00,\n",
       "       -1.23049927e+01,  5.87187195e+00, -9.11688232e+00, -3.19342041e+00,\n",
       "       -1.10015869e-02, -7.16711426e+00])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "model_id = 49  # Update with your model ID\n",
    "version_id = 3  # Update with your version ID\n",
    "\n",
    "@task(name=f'Prediction with Cairo {uuid.uuid4()}')\n",
    "def prediction(image, model_id, version_id):\n",
    "    model = GizaModel(id=49, version=3)\n",
    "\n",
    "    result = model.predict(\n",
    "        input_feed={\"image\": image}, verifiable=True, output_dtype=\"arr_fixed_point\"\n",
    "    )\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "@action(name=f'Execution: Prediction with Cairo {uuid.uuid4()}')\n",
    "def execution():\n",
    "    image = preprocess_image(\"./zero.jpg\")\n",
    "    result = prediction(image, model_id, 1)\n",
    "    print(\"Result: \", result)\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "execution()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO: prover and verifier section (coming soon)**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "giza-actions-mYf3m_Lk-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
